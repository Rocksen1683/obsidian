We have 
$$ M = \alpha\left(P + \frac{1}{R}ed^{T}\right)+ \frac{1-\alpha}{R}(ee^{T})$$
We know that
- $P$ is *sparse*, not all pages are linked together 
- $ed^{T}$ is *dense*, dead end columns 
- $ee^T$ is *fully dense*
By the definition of a [[Google Matrix]], we have 
$$p^{n+1}= Mp^{n}= \alpha Pp^{n} + \frac{\alpha}{R}ed^{T}p^{n} + \frac{1-\alpha}{R}ee^{T}p^{n}$$
We can see that we have $3$ different terms in the RHS 

1. Is a [[Sparse Matrix-Vector Multiplication]] as $P$ is sparse
	- It can be computed efficiently with the above technique
2. Involves $ed^Tp^n$ which is decently expensive 
	- Do $e(d^Tp^n)$ which would be a *dot product* between $d^{T}p^{n}$ which gives a *scalar value*
3.  Involves $ee^Tp^n$ which is ridiculously expensive
	- Instead if we do $e(e^{T}p^{n})$, we would get a *dot-product* between $e^Tp^n$ which would be a *scalar value*
	- $e^Tp^n$ actually results in $1$ as $p^n$ is a *probability vector*
	- This would result in very cheap computation of $e.1$
	- Therefore, we just need to calculate $\frac{1 - \alpha}{R}(e)$ which is cheap as $\frac{1 - \alpha}{R}$ is a *scalar*


So $p^{n+1}$ is just $Mp^{n}$ which would be the summation of the $3$ above steps. This means we actually don't need to build $M$ and it's much cheaper. 
